{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of Data Science\n",
 
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name: Snehaa Ganesan \n",
    "\n",
    "Student Netid: sg4860 \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Case study\n",
    "- Read [this article](http://www.nytimes.com/2012/02/19/magazine/shopping-habits.html) in the New York Times.\n",
    "- Use what we've learned in class and from the book to describe how one could set Target's problem up as a predictive modeling problem, such that they could have gotten the results that they did.  Formulate your solution as a proposed plan using our data science terminology.  Include aspects of the Data Science Workflow that you see as relevant to solving the problem.  Be precise but concise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target's problem as predictive modeling problem, as Data science workflow: \n",
    "\n",
    "Potential questions: \n",
    "\n",
    "a. Has there been sudden changes in any customer's recent buying patterns? \n",
    "b. Are the customers now buying any of these specific items like pregnancy clothing, prenatal supplements, crib, strollers, etc.? Are answers to these questions \n",
    "\n",
    "\n",
    "Target Variable:\n",
    "\n",
    "a. Main target in this problem is to determine if a customer is pregnant.\n",
    "b. Additional target variable here may be to find a correlation. i.e. If search history suggests pregnant, how well does that information correlate with the 'pregnan' prtedicted from buying patterns and recent purchases.\n",
    "\n",
    "Getting the Data: \n",
    "\n",
    "Customer data is acquired when a customer makes a purchase, linked to their unique customer ID. Additional details like customer's personal details and demographics may be acquired as they sign up for Target's loyalty card / membership. Additional details like employment, owning of house/car etc. may be acquired through 3rd party sources ike facebook, email when users give permission to access them. The customer data is most likely stored as form of relational databases for ease of use.\n",
    "\n",
    "Data Pre-Processing/Handling: \n",
    "\n",
    "a. Cleaning the data to remove/fill missing NaN, and null values appropriately, increases consistency in data, reduces processing time by removal of irrelevant data entries (NaN). Thus making the Target's customer data cleaner than before.\n",
    "b. Data Munging and Data Integration are usuallly required to model the data specific to our problem/goal, usually by combining relevant aspects  of customer data. For example, Combining buying data with buyer's recent search history (Best Schools, Child Birth Doctors, pregnancy blog articles read, etc) can be more aiding in the problem/goal domain.\n",
    "\n",
    "The Data's Structure:\n",
    "\n",
    "Having a main target variable our data for the problem is of Supervised structure, of multivariate type. \n",
    "\n",
    "Model/Tool Fitting:\n",
    "\n",
    "a. Fitting the data in a correlation model for correlation between past and recent purchases will tell us if there has been a drastic change in buying patterns/purchases of the any customer.\n",
    "b. Using a Regression model to predict 'pregnant or not' from 'maternity purchases' (recent purchases) and similarly from 'search history' (pregnancy, childbirth, child (school,doctors) related searches). \n",
    "c. Then finding correlation between the prediction from both cases (of b.) will further narrow down our most probable pregnant customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Exploring data in the command line\n",
    "For this part we will be using the data file located in `\"data/advertising_events.csv\"`. This file consists of records that pertain to some online advertising events on a given day. There are 4 comma separated columns in this order: `userid`, `timestamp`, `domain`, and `action`. These fields are of type `int` (continuous), `int` (continuous), `string`, and `int` (category) respectively. Answer the following questions using Linux/Unix bash commands. All questions can be answered in one line (sometimes, with pipes)! Some questions will have many possible solutions. Don't forget that in IPython notebooks you must prefix all bash commands with an exclamation point, i.e. `\"!command arguments\"`.\n",
    "\n",
    "[Hints: You can experiment with whatever you want in the notebook and then delete things to construct your answer later.  You can also use ssh to use the actual bash shell on EC2 and then just paste your answers here. Recall that once you enter the \"!\" then filename completion should work.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. How many records (lines) are in this file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   10341\r\n"
     ]
    }
   ],
   "source": [
    "!grep -v \"\\\\n\" advertising_events.csv | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. How many unique users are in this file? (hint: consider the 'cut' command and use pipe operator '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     732\r\n"
     ]
    }
   ],
   "source": [
    "!cut -f1 -d, advertising_events.csv | sort | uniq | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Rank all domains by the number of visits they received in descending order. (hint: consider the 'cut', 'uniq' and 'sort' commands and the pipe operator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3114 google.com\r\n",
      "2092 facebook.com\r\n",
      "1036 youtube.com\r\n",
      "1034 yahoo.com\r\n",
      "1022 baidu.com\r\n",
      " 513 wikipedia.org\r\n",
      " 511 amazon.com\r\n",
      " 382 qq.com\r\n",
      " 321 twitter.com\r\n",
      " 316 taobao.com\r\n"
     ]
    }
   ],
   "source": [
    "cat advertising_events.csv | cut -f3 -d, | sort | uniq -c | sort -rn \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. List all records for the user with user id 37. (hint: this can be done using 'grep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37,648061658,google.com,0\r\n",
      "37,642479972,google.com,2\r\n",
      "37,644493341,facebook.com,2\r\n",
      "37,654941318,facebook.com,1\r\n",
      "37,649979874,baidu.com,1\r\n",
      "37,653061949,yahoo.com,1\r\n",
      "37,655020469,google.com,3\r\n",
      "37,640878012,amazon.com,0\r\n",
      "37,659864136,youtube.com,1\r\n",
      "37,640361378,yahoo.com,1\r\n",
      "37,653862134,facebook.com,0\r\n",
      "37,648828970,youtube.com,0\r\n"
     ]
    }
   ],
   "source": [
    "!awk -F, '{ if ($1 == 37) print $0 }' advertising_events.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Dealing with data Pythonically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You might find these packages useful. You may import any others you want!\n",
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "from pandas import DataFrame;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Load the data set `\"data/ads_dataset.tsv\"` into a Python Pandas data frame called `ads`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     isbuyer  buy_freq  visit_freq  buy_interval  sv_interval  \\\n",
      "NaN        0       NaN           1           0.0          0.0   \n",
      "NaN        0       NaN           1           0.0          0.0   \n",
      "NaN        0       NaN           1           0.0          0.0   \n",
      "NaN        0       NaN           1           0.0          0.0   \n",
      "NaN        0       NaN           2           0.0          0.5   \n",
      "\n",
      "     expected_time_buy  expected_time_visit  last_buy  last_visit  \\\n",
      "NaN                0.0               0.0000       106         106   \n",
      "NaN                0.0               0.0000        72          72   \n",
      "NaN                0.0               0.0000         5           5   \n",
      "NaN                0.0               0.0000         6           6   \n",
      "NaN                0.0            -101.1493       101         101   \n",
      "\n",
      "     multiple_buy  multiple_visit  uniq_urls  num_checkins  y_buy  \n",
      "NaN             0               0        169          2130      0  \n",
      "NaN             0               0        154          1100      0  \n",
      "NaN             0               0          4            12      0  \n",
      "NaN             0               0        150           539      0  \n",
      "NaN             0               1        103           362      0  \n"
     ]
    }
   ],
   "source": [
    "ads = pd.read_csv('ads_dataset.tsv', sep='\\t');\n",
    "print(ads.head());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Write a Python function called `getDfSummary()` that does the following:\n",
    "- Takes as input a data frame\n",
    "- For each variable in the data frame calculates the following features:\n",
    "  - `number_nan` to count the number of missing not-a-number values\n",
    "  - Ignoring missing, NA, and Null values:\n",
    "    - `number_distinct` to count the number of distinct values a variable can take on\n",
    "    - `mean`, `max`, `min`, `std` (standard deviation), and `25%`, `50%`, `75%` to correspond to the appropriate percentiles\n",
    "- All of these new features should be loaded in a new data frame. Each row of the data frame should be a variable from the input data frame, and the columns should be the new summary features.\n",
    "- Returns this new data frame containing all of the summary information\n",
    "\n",
    "Hint: The pandas `describe()` [(manual page)](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html) method returns a useful series of values that can be used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDfSummary(input_data):\n",
    "    # counting NaN\n",
    "    NaN = len(input_data) - input_data.count()\n",
    "    \n",
    "    #counting unique_values\n",
    "    df = pd.DataFrame()\n",
    "    df['uni'] = pd.Series(df)\n",
    "    df = input_data.apply(pd.Series.nunique)\n",
    "    \n",
    "    #panda's describe() method\n",
    "    input_data2 = input_data.describe(include='all')\n",
    "    \n",
    "    #Transposing it for desired format\n",
    "    output_data = input_data2.transpose()\n",
    "    \n",
    "    #removing the irrelevant field 'count'\n",
    "    del output_data['count']\n",
    "    \n",
    "    #Adding the count of NaN (cNaN), and unique_values fields\n",
    "    output_data['cNaN'] = NaN\n",
    "    output_data['unique_values'] = df\n",
    "    \n",
    "    \n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. How long does it take for your `getDfSummary()` function to work on your `ads` data frame? Show us the results below.\n",
    "\n",
    "Hint: `%timeit getDfSummary(ads)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 70.6 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit getDfSummary(ads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Using the results returned from `getDfSummary()`, which fields, if any, contain missing `NaN` values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['buy_freq']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prints the field names as list that contain NaN values\n",
    "nans = ads.columns[ads.isnull().any()].tolist()\n",
    "df_out = getDfSummary(ads)\n",
    "nans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. For the fields with missing values, does it look like the data is missing at random? Are there any other fields that correlate perfectly, or predict that the data is missing? What would be an appropriate method for filling in missing values?\n",
    "\n",
    "Hint: create another data frame that has just the records with a missing value. Get a summary of this data frame using `getDfSummary()` and compare the differences. Do some feature distributions change dramatically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "isbuyer                1.0\n",
       "buy_freq               1.0\n",
       "visit_freq             1.0\n",
       "buy_interval           1.0\n",
       "sv_interval            1.0\n",
       "expected_time_buy      1.0\n",
       "expected_time_visit    1.0\n",
       "last_buy               1.0\n",
       "last_visit             1.0\n",
       "multiple_buy           1.0\n",
       "multiple_visit         1.0\n",
       "uniq_urls              1.0\n",
       "num_checkins           1.0\n",
       "y_buy                  1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding getDfSummary with only NaN rows/entries\n",
    "NaNd = ads[ads.isnull().any(axis=1)]\n",
    "NaN_data = getDfSummary(NaNd)\n",
    "NaN_data\n",
    "\n",
    "#Observed that the buy_freq data is NaN for 'non buyers'(isbuyer being 0).\n",
    "#Hence, buy_freq, buy_interval, expected_time_buy, multiple_buy all change to 0,showing data for just the non buyers. \n",
    "\n",
    "#Next, I tried filling the missing values with mean,std,median and mode. \n",
    "\n",
    "#Correlation was found between ads (input_data) with NaN_data and between their respective getDfSummary datas, \n",
    "#and found that Filling NaN with mode gives better correlation between them in both cases than median. Median better than Mean. \n",
    "#Mean and std give same correlation. Mean and std give better correlation than 0. \n",
    "\n",
    "#Also,filling in 0 is illogical,as 0 and NaN for buy_freq essentially mean the same but disturb the other fields.\n",
    "# Hence, filling with 0 is unneccesary and wrongful in this scenario.\n",
    "\n",
    "#Filling in with mode, median, mean, std, 0\n",
    "adsz_mode = ads.fillna(ads.mode())\n",
    "adsz_median = ads.fillna(ads.median())\n",
    "adsz_mean = ads.fillna(ads.mean())\n",
    "adsz_std = ads.fillna(ads.std())\n",
    "adsz_0 = ads.fillna((0))\n",
    "\n",
    "#Finding getDfSummary - ies\n",
    "df_mode = getDfSummary(adsz_mode)\n",
    "df_median = getDfSummary(adsz_median)\n",
    "df_mean = getDfSummary(adsz_mean)\n",
    "df_std = getDfSummary(adsz_std)\n",
    "df_0 = getDfSummary(adsz_0)\n",
    "\n",
    "#The correlations are done and shown below\n",
    "\n",
    "#filled with Mode - correlations \n",
    "df_out.corrwith(df_mode)\n",
    "ads.corrwith(adsz_mode)\n",
    "\n",
    "\n",
    "#df_out.corrwith(df_median)\n",
    "#df_out.corrwith(df_std)\n",
    "\n",
    "\n",
    "#filled with mean correlations\n",
    "#df_out.corrwith(df_mean)\n",
    "#ads.corrwith(adsz_mean)\n",
    "\n",
    "\n",
    "#filled with 0 - correlations \n",
    "#df_out.corrwith(df_0)\n",
    "#ads.corrwith(adsz_0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Which variables are binary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['isbuyer', 'multiple_buy', 'multiple_visit', 'y_buy']\n"
     ]
    }
   ],
   "source": [
    "binary_fields = [col for col in ads \n",
    "             if ads[[col]].dropna().isin([0, 1]).all().values]\n",
    "\n",
    "print(binary_fields)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
